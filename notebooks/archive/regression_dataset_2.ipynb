{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4221e4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s6/68z2fm6j28jfpyn0vqv3j9fw0000gn/T/ipykernel_48160/2202352679.py:119: PerformanceWarning: Resolving the schema of a LazyFrame is a potentially expensive operation. Use `LazyFrame.collect_schema()` to get the schema without this warning.\n",
      "  available = set(lf.schema.keys())\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel ist beim Ausführen von Code in der aktuellen Zelle oder einer vorherigen Zelle abgestürzt. \n",
      "\u001b[1;31mBitte überprüfen Sie den Code in der/den Zelle(n), um eine mögliche Fehlerursache zu identifizieren. \n",
      "\u001b[1;31mKlicken Sie <a href='https://aka.ms/vscodeJupyterKernelCrash'>hier</a>, um weitere Informationen zu erhalten. \n",
      "\u001b[1;31mWeitere Informationen finden Sie unter Jupyter <a href='command:jupyter.viewOutput'>Protokoll</a>."
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "from pathlib import Path\n",
    "import math\n",
    "import time\n",
    "\n",
    "# ============================\n",
    "# CONFIG\n",
    "# ============================\n",
    "\n",
    "DATA_ROOT = Path(\"/Users/jakobschneider/Machine Learning/Data_LCC\")\n",
    "\n",
    "ENRICHED_PATH = DATA_ROOT / \"AIS_2024_enriched_optimized.parquet\"\n",
    "MRV_PATH      = DATA_ROOT / \"MRV_2024.xlsx\"\n",
    "\n",
    "OUT_DIR = DATA_ROOT / \"feature_pipeline_2024\"\n",
    "OUT_BUCKET_DIR = OUT_DIR / \"buckets_agg\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_BUCKET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "YEAR = 2024\n",
    "\n",
    "# Bucket count: higher = smaller per-bucket workload (safer)\n",
    "N_BUCKETS = 512  # if kernel still dies, increase to 512\n",
    "\n",
    "# Segment filters / thresholds (documented assumptions)\n",
    "MAX_DT_SECONDS = 2 * 3600    # max 2h gap\n",
    "MAX_SPEED_KN   = 35          # implied speed teleport filter\n",
    "\n",
    "# Quality thresholds\n",
    "MIN_DISTANCE_NM = 500\n",
    "MIN_POINTS      = 10_000\n",
    "MAX_MEDIAN_DT   = 600        # 10 minutes\n",
    "\n",
    "# Movement thresholds\n",
    "MOVING_SOG_KN = 1.0\n",
    "IDLE_SOG_KN   = 0.5\n",
    "\n",
    "\n",
    "# ============================\n",
    "# HELPERS\n",
    "# ============================\n",
    "\n",
    "def haversine_nm(lat1: pl.Expr, lon1: pl.Expr, lat2: pl.Expr, lon2: pl.Expr) -> pl.Expr:\n",
    "    \"\"\"Haversine distance in nautical miles (nm) using Polars Expr methods.\"\"\"\n",
    "    R_km = 6371.0\n",
    "    deg2rad = math.pi / 180.0\n",
    "\n",
    "    lat1r = lat1 * deg2rad\n",
    "    lon1r = lon1 * deg2rad\n",
    "    lat2r = lat2 * deg2rad\n",
    "    lon2r = lon2 * deg2rad\n",
    "\n",
    "    dlat = lat2r - lat1r\n",
    "    dlon = lon2r - lon1r\n",
    "\n",
    "    a = (dlat / 2).sin() ** 2 + lat1r.cos() * lat2r.cos() * (dlon / 2).sin() ** 2\n",
    "    c = (a.sqrt()).arcsin() * 2.0\n",
    "\n",
    "    km = c * R_km\n",
    "    nm = km / 1.852\n",
    "    return nm\n",
    "\n",
    "\n",
    "def dt_seconds_expr(dt_col: pl.Expr) -> pl.Expr:\n",
    "    \"\"\"\n",
    "    Version-stable dt seconds extractor.\n",
    "    - Some Polars versions support .dt.total_seconds()\n",
    "    - Others need .dt.seconds() (may lose days if gaps are huge, but we filter gaps anyway)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Newer Polars\n",
    "        return dt_col.dt.total_seconds()\n",
    "    except Exception:\n",
    "        # Older Polars fallback (ok because we filter dt < 2h)\n",
    "        return dt_col.dt.seconds()\n",
    "\n",
    "\n",
    "# ============================\n",
    "# STEP 0: Define the minimal column set\n",
    "# (Assuming enriched file already contains Length/Width/Draft/VesselType etc.)\n",
    "# ============================\n",
    "\n",
    "# Adjust these names if your columns differ\n",
    "COL_IMO  = \"IMO\"\n",
    "COL_T    = \"BaseDateTime\"\n",
    "COL_LAT  = \"LAT\"\n",
    "COL_LON  = \"LON\"\n",
    "COL_SOG  = \"SOG\"\n",
    "\n",
    "# Optional ship metadata columns already in enriched file\n",
    "# Keep only what you actually want in the model table\n",
    "META_COLS = [\n",
    "    \"VesselType\",\n",
    "    \"Length\",\n",
    "    \"Width\",\n",
    "    \"Draft\",\n",
    "]\n",
    "\n",
    "# Some files have slightly different names; keep only those that exist later\n",
    "BASE_COLS = [COL_IMO, COL_T, COL_LAT, COL_LON, COL_SOG] + META_COLS\n",
    "\n",
    "\n",
    "# ============================\n",
    "# STEP 1: Per-bucket aggregation\n",
    "# ============================\n",
    "\n",
    "for b in range(N_BUCKETS):\n",
    "    t0 = time.time()\n",
    "    out_path = OUT_BUCKET_DIR / f\"agg_bucket_{b:03d}.parquet\"\n",
    "    if out_path.exists():\n",
    "        print(f\"[bucket {b:03d}] exists -> skip\")\n",
    "        continue\n",
    "\n",
    "    # --- Load lazily and reduce early ---\n",
    "    lf = pl.scan_parquet(ENRICHED_PATH)\n",
    "\n",
    "    # --- Keep only columns that exist (prevents KeyError if Draft/VesselType missing) ---\n",
    "    #     Note: LazyFrame schema is available without reading full data.\n",
    "    available = set(lf.schema.keys())\n",
    "    cols = [c for c in BASE_COLS if c in available]\n",
    "\n",
    "    lf = (\n",
    "        lf\n",
    "        .select(cols)\n",
    "        .filter(\n",
    "            pl.col(COL_IMO).is_not_null() &\n",
    "            pl.col(COL_T).is_not_null() &\n",
    "            pl.col(COL_LAT).is_not_null() &\n",
    "            pl.col(COL_LON).is_not_null() &\n",
    "            (pl.col(COL_SOG) >= 0)\n",
    "        )\n",
    "        # --- Hash bucket on IMO to process a manageable subset ---\n",
    "        .with_columns(\n",
    "            bucket=(pl.col(COL_IMO).hash() % N_BUCKETS).cast(pl.Int16)\n",
    "        )\n",
    "        .filter(pl.col(\"bucket\") == b)\n",
    "        .drop(\"bucket\")\n",
    "    )\n",
    "\n",
    "    # --- Sort within bucket (still heavy, but now bounded) ---\n",
    "    lf = lf.sort([COL_IMO, COL_T])\n",
    "\n",
    "    # --- dt seconds per IMO ---\n",
    "    lf = lf.with_columns(\n",
    "        dt_seconds=dt_seconds_expr(pl.col(COL_T).diff().over(COL_IMO))\n",
    "    ).filter(\n",
    "        pl.col(\"dt_seconds\").is_not_null() &\n",
    "        (pl.col(\"dt_seconds\") > 0) &\n",
    "        (pl.col(\"dt_seconds\") < MAX_DT_SECONDS)\n",
    "    )\n",
    "\n",
    "    # --- Segment distance + implied speed teleport filter ---\n",
    "    lf = lf.with_columns(\n",
    "        segment_distance_nm=haversine_nm(\n",
    "            pl.col(COL_LAT).shift(1).over(COL_IMO),\n",
    "            pl.col(COL_LON).shift(1).over(COL_IMO),\n",
    "            pl.col(COL_LAT),\n",
    "            pl.col(COL_LON),\n",
    "        )\n",
    "    ).with_columns(\n",
    "        implied_speed_kn=pl.col(\"segment_distance_nm\") / (pl.col(\"dt_seconds\") / 3600.0)\n",
    "    ).filter(\n",
    "        pl.col(\"segment_distance_nm\").is_not_null() &\n",
    "        (pl.col(\"segment_distance_nm\") >= 0) &\n",
    "        (pl.col(\"implied_speed_kn\") < MAX_SPEED_KN)\n",
    "    )\n",
    "\n",
    "    # --- Movement flags ---\n",
    "    lf = lf.with_columns([\n",
    "        (pl.col(COL_SOG) > MOVING_SOG_KN).cast(pl.Int8).alias(\"moving_flag\"),\n",
    "        (pl.col(COL_SOG) < IDLE_SOG_KN).cast(pl.Int8).alias(\"idle_flag\"),\n",
    "    ])\n",
    "\n",
    "    # --- Aggregate to IMO (one row per ship) ---\n",
    "    # Important: use Expr.sum() for expression sums to avoid older Polars errors.\n",
    "    aggs = [\n",
    "        pl.sum(\"segment_distance_nm\").alias(\"ais_distance_nm_total\"),\n",
    "        (pl.sum(\"dt_seconds\") / 3600.0).alias(\"ais_time_hours_total\"),\n",
    "        pl.col(COL_IMO).count().alias(\"ais_points\"),\n",
    "\n",
    "        pl.mean(COL_SOG).alias(\"sog_mean_kn\"),\n",
    "        pl.quantile(COL_SOG, 0.50).alias(\"sog_p50_kn\"),\n",
    "        pl.quantile(COL_SOG, 0.95).alias(\"sog_p95_kn\"),\n",
    "\n",
    "        ((pl.col(\"dt_seconds\") * pl.col(\"moving_flag\")).sum() / 3600.0).alias(\"moving_hours\"),\n",
    "        ((pl.col(\"dt_seconds\") * pl.col(\"idle_flag\")).sum() / 3600.0).alias(\"idle_hours\"),\n",
    "\n",
    "        pl.median(\"dt_seconds\").alias(\"median_dt_seconds\"),\n",
    "    ]\n",
    "\n",
    "    # Metadata: choose robust aggregation per IMO (median/first)\n",
    "    if \"VesselType\" in available:\n",
    "        aggs.append(pl.first(\"VesselType\").alias(\"VesselType\"))\n",
    "    if \"Length\" in available:\n",
    "        aggs.append(pl.median(\"Length\").alias(\"Length\"))\n",
    "    if \"Width\" in available:\n",
    "        aggs.append(pl.median(\"Width\").alias(\"Width\"))\n",
    "    if \"Draft\" in available:\n",
    "        aggs.append(pl.median(\"Draft\").alias(\"draft_m_median\"))\n",
    "\n",
    "    lf_agg = (\n",
    "        lf\n",
    "        .group_by(COL_IMO)\n",
    "        .agg(aggs)\n",
    "        .with_columns([\n",
    "            pl.lit(YEAR).alias(\"year\"),\n",
    "            (pl.col(\"moving_hours\") / pl.col(\"ais_time_hours_total\")).alias(\"moving_share\"),\n",
    "            (\n",
    "                (pl.col(\"ais_distance_nm_total\") > MIN_DISTANCE_NM) &\n",
    "                (pl.col(\"ais_points\") > MIN_POINTS) &\n",
    "                (pl.col(\"median_dt_seconds\") < MAX_MEDIAN_DT)\n",
    "            ).alias(\"quality_ok\")\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    # --- Collect only aggregated result (small!) and write to disk ---\n",
    "    df_bucket = lf_agg.collect()\n",
    "    df_bucket.write_parquet(out_path)\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    print(f\"[bucket {b:03d}] rows={df_bucket.shape[0]} written -> {out_path.name} ({dt:.1f}s)\")\n",
    "\n",
    "print(\"Bucket aggregation finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e433b3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_ROOT = Path(\"/Users/jakobschneider/Machine Learning/Data_LCC\")\n",
    "OUT_DIR = DATA_ROOT / \"feature_pipeline_2024\"\n",
    "OUT_BUCKET_DIR = OUT_DIR / \"buckets_agg\"\n",
    "\n",
    "agg_files = sorted(OUT_BUCKET_DIR.glob(\"agg_bucket_*.parquet\"))\n",
    "print(\"Agg files:\", len(agg_files))\n",
    "\n",
    "df_features = pl.concat([pl.read_parquet(p) for p in agg_files], how=\"vertical\")\n",
    "\n",
    "# Optional: remove duplicates just in case (should not happen with hashing, but safe)\n",
    "df_features = df_features.unique(subset=[\"IMO\", \"year\"], keep=\"first\")\n",
    "\n",
    "out_features_path = OUT_DIR / \"features_imo_2024.parquet\"\n",
    "df_features.write_parquet(out_features_path)\n",
    "\n",
    "print(df_features.shape)\n",
    "print(\"Saved:\", out_features_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8781c7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_ROOT = Path(\"/Users/jakobschneider/Machine Learning/Data_LCC\")\n",
    "OUT_DIR = DATA_ROOT / \"feature_pipeline_2024\"\n",
    "\n",
    "MRV_PATH = DATA_ROOT / \"MRV_2024.xlsx\"\n",
    "FEATURES_PATH = OUT_DIR / \"features_imo_2024.parquet\"\n",
    "\n",
    "YEAR = 2024\n",
    "\n",
    "df_features = pl.read_parquet(FEATURES_PATH)\n",
    "\n",
    "df_mrv = (\n",
    "    pl.read_excel(MRV_PATH)\n",
    "    .select([\n",
    "        pl.col(\"IMO Number\").alias(\"IMO\"),\n",
    "        pl.col(\"Reporting Period\").alias(\"year\"),\n",
    "        pl.col(\"CO₂ emissions per distance [kg CO₂ / n mile]\").alias(\"y_co2_per_nm_kg\"),\n",
    "    ])\n",
    "    .filter(pl.col(\"year\") == YEAR)\n",
    ")\n",
    "\n",
    "df_model = (\n",
    "    df_features\n",
    "    .join(df_mrv, on=[\"IMO\", \"year\"], how=\"inner\")\n",
    ")\n",
    "\n",
    "out_model_path = OUT_DIR / \"model_table_imo_2024.parquet\"\n",
    "df_model.write_parquet(out_model_path)\n",
    "\n",
    "print(df_model.shape)\n",
    "print(\"Saved:\", out_model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
